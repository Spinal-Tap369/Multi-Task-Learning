{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import higher\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'E:/Py_Files/cold/archive'\n",
    "\n",
    "# List of data files\n",
    "data_files = [f'combined_data_{i}.txt' for i in range(1, 5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to be processed: ['combined_data_1.txt']\n"
     ]
    }
   ],
   "source": [
    "data_files = [data_files[0]]  # Process only combined_data_1.txt\n",
    "print(f'Files to be processed: {data_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Initialize Sets to Store Unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids = set()\n",
    "unique_movie_ids = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll read the data files line by line and collect user and movie IDs without storing the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unique IDs from combined_data_1.txt...\n"
     ]
    }
   ],
   "source": [
    "for file in data_files:\n",
    "    print(f'Collecting unique IDs from {file}...')\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        movie_id = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.endswith(':'):\n",
    "                # Movie ID line\n",
    "                movie_id = int(line.replace(':', ''))\n",
    "                unique_movie_ids.add(movie_id)\n",
    "            else:\n",
    "                # User data line\n",
    "                user_id_str, rating_str, date = line.split(',')\n",
    "                user_id = int(user_id_str)\n",
    "                unique_user_ids.add(user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique users: 470758\n",
      "Total unique movies: 4499\n"
     ]
    }
   ],
   "source": [
    "unique_user_ids = list(unique_user_ids)\n",
    "unique_movie_ids = list(unique_movie_ids)\n",
    "print(f'Total unique users: {len(unique_user_ids)}')\n",
    "print(f'Total unique movies: {len(unique_movie_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.35  # 35% for training\n",
    "test_ratio = 0.05   # 5% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training users: 164765\n",
      "Number of testing users: 23537\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Shuffle and sample users\n",
    "random.shuffle(unique_user_ids)\n",
    "num_train_users = int(train_ratio * len(unique_user_ids))\n",
    "num_test_users = int(test_ratio * len(unique_user_ids))\n",
    "\n",
    "train_users = set(unique_user_ids[:num_train_users])\n",
    "test_users = set(unique_user_ids[num_train_users:num_train_users + num_test_users])\n",
    "\n",
    "print(f'Number of training users: {len(train_users)}')\n",
    "print(f'Number of testing users: {len(test_users)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected users: 188302\n"
     ]
    }
   ],
   "source": [
    "selected_users = train_users.union(test_users)\n",
    "user_id_map = {user_id: idx for idx, user_id in enumerate(selected_users)}\n",
    "num_users = len(user_id_map)\n",
    "print(f'Number of selected users: {num_users}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies: 4499\n"
     ]
    }
   ],
   "source": [
    "movie_id_map = {movie_id: idx for idx, movie_id in enumerate(unique_movie_ids)}\n",
    "num_movies = len(movie_id_map)\n",
    "print(f'Number of movies: {num_movies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_ids = []\n",
    "train_movie_ids = []\n",
    "train_preferences = []\n",
    "\n",
    "test_user_ids = []\n",
    "test_movie_ids = []\n",
    "test_preferences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing combined_data_1.txt for data collection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24058263it [00:33, 713225.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in data_files:\n",
    "    print(f'Processing {file} for data collection...')\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        movie_id = None\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip()\n",
    "            if line.endswith(':'):\n",
    "                # Movie ID line\n",
    "                movie_id_raw = int(line.replace(':', ''))\n",
    "                movie_id_enc = movie_id_map.get(movie_id_raw)\n",
    "                if movie_id_enc is None:\n",
    "                    continue  # Skip movies not in our mapping (should not happen)\n",
    "            else:\n",
    "                # User data line\n",
    "                user_id_str, rating_str, date = line.split(',')\n",
    "                user_id_raw = int(user_id_str)\n",
    "                if user_id_raw in user_id_map:\n",
    "                    user_id_enc = user_id_map[user_id_raw]\n",
    "                    rating = int(rating_str)\n",
    "                    preference = 1 if rating > 3 else 0\n",
    "                    if user_id_raw in train_users:\n",
    "                        train_user_ids.append(user_id_enc)\n",
    "                        train_movie_ids.append(movie_id_enc)\n",
    "                        train_preferences.append(preference)\n",
    "                    elif user_id_raw in test_users:\n",
    "                        test_user_ids.append(user_id_enc)\n",
    "                        test_movie_ids.append(movie_id_enc)\n",
    "                        test_preferences.append(preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection completed.\n",
      "Training data shape: (8424744, 3)\n",
      "Testing data shape: (1202982, 3)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "train_data = pd.DataFrame({\n",
    "    'UserID_enc': train_user_ids,\n",
    "    'MovieID_enc': train_movie_ids,\n",
    "    'Preference': train_preferences\n",
    "})\n",
    "\n",
    "# Testing data\n",
    "test_data = pd.DataFrame({\n",
    "    'UserID_enc': test_user_ids,\n",
    "    'MovieID_enc': test_movie_ids,\n",
    "    'Preference': test_preferences\n",
    "})\n",
    "\n",
    "print('Data collection completed.')\n",
    "print(f'Training data shape: {train_data.shape}')\n",
    "print(f'Testing data shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tasks for Meta-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training users with data: 164765\n"
     ]
    }
   ],
   "source": [
    "# Group the training data by user\n",
    "user_groups = train_data.groupby('UserID_enc')\n",
    "print(f'Number of training users with data: {len(user_groups)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Support and Query Set Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_support = 5   # Number of samples in the support set per user\n",
    "K_query = 15    # Number of samples in the query set per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks created: 90995\n"
     ]
    }
   ],
   "source": [
    "tasks = []\n",
    "\n",
    "for user_id, group in user_groups:\n",
    "    if len(group) >= (K_support + K_query):\n",
    "        # Shuffle the user's data\n",
    "        user_data = group.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        # Split into support and query sets\n",
    "        support_set = user_data.iloc[:K_support]\n",
    "        query_set = user_data.iloc[K_support:K_support + K_query]\n",
    "        tasks.append({'user_id': user_id, 'support_set': support_set, 'query_set': query_set})\n",
    "    else:\n",
    "        # Skip users with insufficient data\n",
    "        continue\n",
    "\n",
    "print(f'Number of tasks created: {len(tasks)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MetaCS-DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaCSDNN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size=64, hidden_sizes=[64, 32]):\n",
    "        super(MetaCSDNN, self).__init__()\n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc_input_size = embedding_size * 2\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.fc_layers.append(nn.Linear(fc_input_size, hidden_size))\n",
    "            fc_input_size = hidden_size  # Update input size for next layer\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(fc_input_size, 1)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Embeddings\n",
    "        user_embeds = self.user_embedding(user_ids)\n",
    "        item_embeds = self.item_embedding(item_ids)\n",
    "        x = torch.cat([user_embeds, item_embeds], dim=1)\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = fc_layer(x)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # Output layer with sigmoid activation\n",
    "        x = self.output_layer(x)\n",
    "        out = torch.sigmoid(x)\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaCSDNN(\n",
      "  (user_embedding): Embedding(188302, 64)\n",
      "  (item_embedding): Embedding(4499, 64)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "embedding_size = 64\n",
    "hidden_sizes = [64, 32]\n",
    "\n",
    "# Instantiate the model\n",
    "num_users_model = num_users  # Number of selected users\n",
    "num_items_model = num_movies  # Number of movies in the data\n",
    "\n",
    "model = MetaCSDNN(num_users=num_users_model, num_items=num_items_model, embedding_size=embedding_size, hidden_sizes=hidden_sizes)\n",
    "\n",
    "# Move the model to device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTaskDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.user_ids = data['UserID_enc'].values\n",
    "        self.item_ids = data['MovieID_enc'].values\n",
    "        self.labels = data['Preference'].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.user_ids[idx]\n",
    "        item_id = self.item_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(user_id, dtype=torch.long), torch.tensor(item_id, dtype=torch.long), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 132734,\n",
       " 'support_set':    UserID_enc  MovieID_enc  Preference\n",
       " 0      132734         2111           0\n",
       " 1      132734           27           0\n",
       " 2      132734         1541           0\n",
       " 3      132734         2127           0\n",
       " 4      132734         1323           0,\n",
       " 'query_set':     UserID_enc  MovieID_enc  Preference\n",
       " 5       132734         1917           1\n",
       " 6       132734          562           1\n",
       " 7       132734         3196           1\n",
       " 8       132734          196           1\n",
       " 9       132734          657           1\n",
       " 10      132734         3078           1\n",
       " 11      132734         2170           1\n",
       " 12      132734         3924           1\n",
       " 13      132734         2199           0\n",
       " 14      132734         3937           1\n",
       " 15      132734         2861           1\n",
       " 16      132734         4122           0\n",
       " 17      132734         3824           0\n",
       " 18      132734         3264           0\n",
       " 19      132734         2469           0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_task = tasks[0]\n",
    "first_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support set DataLoader\n",
    "support_dataset = UserTaskDataset(first_task['support_set'])\n",
    "support_loader = DataLoader(support_dataset, batch_size=K_support, shuffle=True)\n",
    "\n",
    "# Query set DataLoader\n",
    "query_dataset = UserTaskDataset(first_task['query_set'])\n",
    "query_loader = DataLoader(query_dataset, batch_size=K_query, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "meta_lr = 0.0001       # Learning rate for meta-optimizer (outer loop)\n",
    "inner_lr = 0.001      # Learning rate for task-specific learner (inner loop)\n",
    "num_inner_steps = 15  # Number of inner-loop updates per task\n",
    "num_epochs = 4       # Number of epochs for meta-training\n",
    "start_epoch = 0\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Initialize the meta-optimizer\n",
    "meta_optimizer = optim.Adam(model.parameters(), lr=meta_lr)\n",
    "\n",
    "inner_optimizer = torch.optim.SGD(model.parameters(), lr=inner_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store losses for plotting or analysis\n",
    "all_epoch_losses = []\n",
    "all_support_losses = []\n",
    "all_query_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint directory\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAM\\AppData\\Local\\Temp\\ipykernel_25556\\1377023360.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 3\n"
     ]
    }
   ],
   "source": [
    "# Check for existing checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'meta_model_checkpoint.pth')\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print('Loading checkpoint...')\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    meta_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
    "    all_epoch_losses = checkpoint['all_epoch_losses']\n",
    "    all_support_losses = checkpoint['all_support_losses']\n",
    "    all_query_losses = checkpoint['all_query_losses']\n",
    "    print(f'Resuming training from epoch {start_epoch+1}')\n",
    "else:\n",
    "    print('No checkpoint found. Starting training from scratch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 - Average Support Loss: 0.5210, Average Query Loss (Meta Loss): 0.5995\n",
      "Checkpoint saved at epoch 3\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 - Average Support Loss: 0.5194, Average Query Loss (Meta Loss): 0.5979\n",
      "Checkpoint saved at epoch 4\n"
     ]
    }
   ],
   "source": [
    "# Begin meta-training\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    epoch_loss = 0.0\n",
    "    epoch_support_loss = 0.0\n",
    "    epoch_query_loss = 0.0\n",
    "\n",
    "    # Optionally, shuffle tasks at the beginning of each epoch\n",
    "    random.shuffle(tasks)\n",
    "\n",
    "    # Create a tqdm progress bar for tasks\n",
    "    task_progress = tqdm(enumerate(tasks), total=len(tasks), desc='Tasks', leave=False)\n",
    "\n",
    "    for task_idx, task in task_progress:\n",
    "        # Get the support and query sets for the current task\n",
    "        support_set = task['support_set']\n",
    "        query_set = task['query_set']\n",
    "\n",
    "        # Create data loaders for support and query sets\n",
    "        support_dataset = UserTaskDataset(support_set)\n",
    "        support_loader = DataLoader(support_dataset, batch_size=K_support, shuffle=True)\n",
    "\n",
    "        query_dataset = UserTaskDataset(query_set)\n",
    "        query_loader = DataLoader(query_dataset, batch_size=K_query, shuffle=True)\n",
    "\n",
    "        # Get the data from the support set\n",
    "        user_ids_supp, item_ids_supp, labels_supp = next(iter(support_loader))\n",
    "        user_ids_supp = user_ids_supp.to(device)\n",
    "        item_ids_supp = item_ids_supp.to(device)\n",
    "        labels_supp = labels_supp.to(device)\n",
    "\n",
    "        # Get the data from the query set\n",
    "        user_ids_query, item_ids_query, labels_query = next(iter(query_loader))\n",
    "        user_ids_query = user_ids_query.to(device)\n",
    "        item_ids_query = item_ids_query.to(device)\n",
    "        labels_query = labels_query.to(device)\n",
    "\n",
    "        # Zero the gradients of the meta-optimizer\n",
    "        meta_optimizer.zero_grad()\n",
    "\n",
    "        # Use higher to create a functional model for inner-loop updates\n",
    "        with higher.innerloop_ctx(model, inner_optimizer, copy_initial_weights=False) as (fmodel, diffopt):\n",
    "            # Inner-loop adaptation\n",
    "            for inner_step in range(num_inner_steps):\n",
    "                # Forward pass on support set\n",
    "                preds_supp = fmodel(user_ids_supp, item_ids_supp)\n",
    "                loss_supp = loss_fn(preds_supp, labels_supp)\n",
    "                # Compute gradients and update the model parameters\n",
    "                diffopt.step(loss_supp)\n",
    "\n",
    "            # Accumulate support loss for logging\n",
    "            epoch_support_loss += loss_supp.item()\n",
    "\n",
    "            # Compute loss on query set with updated parameters\n",
    "            preds_query = fmodel(user_ids_query, item_ids_query)\n",
    "            loss_query = loss_fn(preds_query, labels_query)\n",
    "            # Accumulate query loss for logging\n",
    "            epoch_query_loss += loss_query.item()\n",
    "\n",
    "            # Backpropagate the meta-loss\n",
    "            loss_query.backward()\n",
    "\n",
    "        # Update the meta-parameters\n",
    "        meta_optimizer.step()\n",
    "\n",
    "        # Update the progress bar with task-level losses\n",
    "        task_progress.set_postfix({\n",
    "            'Task': f'{task_idx+1}/{len(tasks)}',\n",
    "            'Support Loss': f'{loss_supp.item():.4f}',\n",
    "            'Query Loss': f'{loss_query.item():.4f}'\n",
    "        })\n",
    "\n",
    "    # Average losses over all tasks\n",
    "    avg_support_loss = epoch_support_loss / len(tasks)\n",
    "    avg_query_loss = epoch_query_loss / len(tasks)\n",
    "    avg_epoch_loss = avg_query_loss  # The meta-loss is based on the query loss\n",
    "\n",
    "    # Store losses for analysis\n",
    "    all_support_losses.append(avg_support_loss)\n",
    "    all_query_losses.append(avg_query_loss)\n",
    "    all_epoch_losses.append(avg_epoch_loss)\n",
    "\n",
    "    # Print epoch-level losses\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Average Support Loss: {avg_support_loss:.4f}, Average Query Loss (Meta Loss): {avg_query_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': meta_optimizer.state_dict(),\n",
    "        'all_epoch_losses': all_epoch_losses,\n",
    "        'all_support_losses': all_support_losses,\n",
    "        'all_query_losses': all_query_losses\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f'Checkpoint saved at epoch {epoch+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Test Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create tasks for the test users, similar to how we did for the training users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test users: 23537\n"
     ]
    }
   ],
   "source": [
    "# Group the test data by user\n",
    "test_user_groups = test_data.groupby('UserID_enc')\n",
    "print(f'Number of test users: {len(test_user_groups)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have only positive samples, we'll adjust our K_support and K_query accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_support = 5    # Number of samples in the support set per user\n",
    "K_query_pos = 15 # Number of positive samples in the query set per user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Test Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need negative samples for evaluation, we'll generate them by sampling items that the user has not interacted with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare a set of negative samples for each user by sampling items they haven't rated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Lists to Store Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Lists to store metrics\n",
    "precision_at_k = {1: [], 3: [], 5: []}\n",
    "auroc_list = []\n",
    "mrr_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_item_ids = set(train_data['MovieID_enc'].unique()).union(set(test_data['MovieID_enc'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(user_id, num_negatives, all_item_ids, user_interacted_items):\n",
    "    # Items the user has not interacted with\n",
    "    non_interacted_items = all_item_ids - user_interacted_items\n",
    "    # Randomly sample negative items\n",
    "    negative_items = random.sample(non_interacted_items, num_negatives)\n",
    "    return negative_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Test Tasks with Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_support = 5    # Number of samples in the support set per user\n",
    "K_query_pos = 15 # Number of positive samples in the query set per user\n",
    "K_query_neg = 15 # Number of negative samples in the query set per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m query_set_pos \u001b[38;5;241m=\u001b[39m pos_samples\u001b[38;5;241m.\u001b[39miloc[K_support:K_support \u001b[38;5;241m+\u001b[39m K_query_pos]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Generate negative samples\u001b[39;00m\n\u001b[0;32m     24\u001b[0m negative_item_ids \u001b[38;5;241m=\u001b[39m generate_negative_samples(\n\u001b[1;32m---> 25\u001b[0m     user_id\u001b[38;5;241m=\u001b[39m\u001b[43muser_id_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m, \n\u001b[0;32m     26\u001b[0m     num_negatives\u001b[38;5;241m=\u001b[39mK_query_neg, \n\u001b[0;32m     27\u001b[0m     all_item_ids\u001b[38;5;241m=\u001b[39mall_item_ids, \n\u001b[0;32m     28\u001b[0m     user_interacted_items\u001b[38;5;241m=\u001b[39muser_interacted_items\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Create negative query set\u001b[39;00m\n\u001b[0;32m     31\u001b[0m query_set_neg \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUserID_enc\u001b[39m\u001b[38;5;124m'\u001b[39m: [user_id] \u001b[38;5;241m*\u001b[39m K_query_neg,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMovieID_enc\u001b[39m\u001b[38;5;124m'\u001b[39m: negative_item_ids,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreference\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m K_query_neg\n\u001b[0;32m     35\u001b[0m })\n",
      "\u001b[1;31mKeyError\u001b[0m: 6"
     ]
    }
   ],
   "source": [
    "test_tasks = []\n",
    "\n",
    "# All unique item IDs\n",
    "all_item_ids = set(train_data['MovieID_enc'].unique()).union(set(test_data['MovieID_enc'].unique()))\n",
    "\n",
    "# Iterate over each user in the test data\n",
    "# Iterate over each user in the test data\n",
    "for user_id, group in test_user_groups:\n",
    "    # Positive samples\n",
    "    pos_samples = group\n",
    "    user_interacted_items = set(pos_samples['MovieID_enc'].unique())\n",
    "    \n",
    "    # Check if the user has enough positive samples and available negative samples\n",
    "    if len(pos_samples) >= (K_support + K_query_pos) and len(user_interacted_items) < len(all_item_ids):\n",
    "        # Shuffle positive samples\n",
    "        pos_samples = pos_samples.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # Create support set\n",
    "        support_set = pos_samples.iloc[:K_support]\n",
    "        \n",
    "        # Create positive query set\n",
    "        query_set_pos = pos_samples.iloc[K_support:K_support + K_query_pos]\n",
    "        \n",
    "        # Generate negative samples\n",
    "        negative_item_ids = generate_negative_samples(\n",
    "            user_id=user_id,  # Pass the encoded user ID directly\n",
    "            num_negatives=K_query_neg, \n",
    "            all_item_ids=all_item_ids, \n",
    "            user_interacted_items=user_interacted_items\n",
    "        )\n",
    "        # Create negative query set\n",
    "        query_set_neg = pd.DataFrame({\n",
    "            'UserID_enc': [user_id] * len(negative_item_ids),\n",
    "            'MovieID_enc': negative_item_ids,\n",
    "            'Preference': [0] * len(negative_item_ids)\n",
    "        })\n",
    "        \n",
    "        # Combine positive and negative query sets\n",
    "        query_set = pd.concat([query_set_pos, query_set_neg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        test_tasks.append({'user_id': user_id, 'support_set': support_set, 'query_set': query_set})\n",
    "    else:\n",
    "        # Skip users with insufficient data or no negative samples\n",
    "        continue\n",
    "\n",
    "print(f'Number of test tasks created: {len(test_tasks)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coldenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
